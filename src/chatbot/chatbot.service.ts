import { Injectable, Logger } from '@nestjs/common'
import { RagService } from '../rag/rag.service'
import { LlmService } from '../llm/llm.service'
import { detectLanguage } from '../common/utils/lang-detect.util'
import { CHATBOT_PROMPT_TEMPLATES } from '../common/prompts/chatbot.prompts'
import { MemoryService } from 'src/memory/memory.service'

/**
 * ChatbotService
 *
 * Main orchestrator for chatbot interactions.
 * Handles message flow, language detection, context retrieval (RAG),
 * prompt building, LLM interaction, and session memory management.
 */
@Injectable()
export class ChatbotService {
  private readonly logger = new Logger(ChatbotService.name)

  /**
   * Constructs the ChatbotService with required dependencies.
   * @param ragService Retrieval-Augmented Generation service (context provider).
   * @param llmService LLM interaction service (model agnostic).
   * @param memoryService Chat memory/session backend.
   */
  constructor(
    private readonly ragService: RagService,
    private readonly llmService: LlmService,
    private readonly memoryService: MemoryService,
  ) {}

  /**
   * Handles a new user chat input.
   *  - Detects input language (if not provided)
   *  - Retrieves chat memory and external context (RAG)
   *  - Builds the LLM prompt in the appropriate language
   *  - Calls the LLM provider and saves both user input and agent reply in memory
   *
   * @param options Options object containing content, connectionId, and optionally userLang.
   * @returns The agent's reply as generated by the LLM.
   */
  async chat(options: { content: string; connectionId: string; userLang?: string }): Promise<string> {
    const { content, connectionId } = options
    let { userLang } = options // userLang puede venir por options, o lo detectamos

    this.logger.debug(`Received user input: "${content}" [connectionId: ${connectionId}]`)

    // Detect language of user input if not provided
    if (!userLang) {
      userLang = detectLanguage(content)
      this.logger.log(`Detected user language: ${userLang}`)
    } else {
      this.logger.log(`Using provided user language: ${userLang}`)
    }

    // Retrieve prior chat history for session
    const history = await this.memoryService.getHistory(connectionId)
    this.logger.debug(`Retrieved history for session: ${JSON.stringify(history)}`)

    // Retrieve additional context (e.g., RAG)
    const contextArr = await this.ragService.retrieveContext(content)
    const context = contextArr.join('\n---\n')
    this.logger.debug(`Retrieved external context: ${JSON.stringify(contextArr)}`)

    // Compose the memory/history block for the prompt
    const historyContext = history.map((m) => `${m.role === 'user' ? 'user' : 'agent'}: ${m.content}`).join('\n')

    // Build prompt using language-appropriate template
    const template = CHATBOT_PROMPT_TEMPLATES[userLang] || CHATBOT_PROMPT_TEMPLATES['en']
    const prompt = template(`${historyContext}\n${context}`, content)
    this.logger.verbose(`Final prompt built for LLM:\n${prompt}`)

    // Call LLM to get response
    const answer = await this.llmService.generate(prompt, { userLang })
    this.logger.debug(`LLM generated answer: "${answer}"`)

    // Save both user question and agent reply to memory
    await this.memoryService.addMessage(connectionId, 'user', content)
    await this.memoryService.addMessage(connectionId, 'assistant', answer)
    this.logger.log(`History updated for session: ${connectionId}`)

    return answer
  }
}
