import { Injectable, Logger } from '@nestjs/common'
import { LlmService } from '../llm/llm.service'
import { CHATBOT_PROMPT_TEMPLATES } from '../common/prompts/chatbot.prompts'
import { SessionEntity } from 'src/core/models'

/**
 * ChatbotService
 *
 * Main orchestrator for chatbot interactions.
 * Handles message flow, language detection, prompt building, and LLM interaction.
 * Chat memory and RAG are handled internally by LlmService via LangChain Memory and tools.
 */
@Injectable()
export class ChatbotService {
  private readonly logger = new Logger(ChatbotService.name)

  /**
   * Constructs the ChatbotService with required dependencies.
   * @param llmService LLM interaction service (model agnostic).
   */
  constructor(private readonly llmService: LlmService) {}

  /**
   * Handles a new user chat input.
   *  - Detects input language (if not provided)
   *  - Builds the LLM prompt in the appropriate language
   *  - Calls the LLM provider (which manages memory and RAG internally)
   *
   * @param options Options object containing userInput and session.
   * @returns The agent's reply as generated by the LLM.
   */
  async chat(options: { userInput: string; session: SessionEntity }): Promise<string> {
    const { userInput, session } = options
    const { lang, connectionId, userName } = session

    this.logger.debug(`Received user input: "${userInput}" [connectionId: ${connectionId}]`)

    // Detect language of user input (or fallback to session lang)
    let userLang = await this.llmService.detectLanguage(userInput)

    if (!userLang && lang) {
      userLang = lang
      this.logger.log(`Detected user language: ${userLang}`)
    } else {
      this.logger.log(`Using detected user language: ${userLang}`)
    }

    // Build prompt using language-appropriate template.
    const template = CHATBOT_PROMPT_TEMPLATES[userLang] || CHATBOT_PROMPT_TEMPLATES['en']
    const prompt = template('', userInput, userName)
    this.logger.verbose(`Final prompt built for LLM:\n${prompt}`)

    // Call LLM to get response (memory + RAG via tools se manejan dentro de LlmService)
    const answer = await this.llmService.generate(prompt, { session })
    this.logger.debug(`LLM generated answer: "${answer}"`)

    this.logger.log(`Chat handled via LlmService (memory + RAG tools) for session: ${connectionId}`)

    return answer
  }
}
