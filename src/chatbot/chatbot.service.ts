import { Injectable, Logger } from '@nestjs/common'
import { RagService } from '../rag/rag.service'
import { LlmService } from '../llm/llm.service'
import { MemoryService } from 'src/memory/memory.service'
import { SessionEntity } from 'src/core/models'
import { AgentContentService } from 'src/core/agent-content.service'

/**
 * ChatbotService
 *
 * Main orchestrator for chatbot interactions.
 * Handles message flow, language detection, context retrieval (RAG),
 * prompt building, LLM interaction, and session memory management.
 */
@Injectable()
export class ChatbotService {
  private readonly logger = new Logger(ChatbotService.name)

  /**
   * Constructs the ChatbotService with required dependencies.
   * @param ragService Retrieval-Augmented Generation service (context provider).
   * @param llmService LLM interaction service (model agnostic).
   * @param memoryService Chat memory/session backend.
   */
  constructor(
    private readonly ragService: RagService,
    private readonly llmService: LlmService,
    private readonly memoryService: MemoryService,
    private readonly agentContent: AgentContentService,
  ) {}

  /**
   * Handles a new user chat input.
   *  - Detects input language (if not provided)
   *  - Retrieves chat memory and external context (RAG)
   *  - Builds the LLM prompt in the appropriate language
   *  - Calls the LLM provider and saves both user input and agent reply in memory
   *
   * @param options Options object containing userInput, connectionId, and optionally userLang.
   * @returns The agent's reply as generated by the LLM.
   */
  async chat(options: { userInput: string; session: SessionEntity }): Promise<string> {
    const { userInput, session } = options
    const { lang, connectionId, userName } = session

    this.logger.debug(`Received user input: "${userInput}" [connectionId: ${connectionId}]`)

    let userLang = await this.llmService.detectLanguage(userInput)

    // Detect language of user input if not provided
    if (!userLang && lang) {
      userLang = lang
      this.logger.log(`Detected user language: ${userLang}`)
    } else {
      this.logger.log(`Using provided user language: ${userLang}`)
    }

    // Retrieve prior chat history for session
    const history = await this.memoryService.getHistory(connectionId)
    this.logger.debug(`Retrieved history for session: ${JSON.stringify(history)}`)

    // Retrieve additional context (e.g., RAG)
    this.logger.log(`[RAG] Consulting vector store for context...`)
    const contextArr = await this.ragService.retrieveContext(userInput)
    this.logger.log(`[RAG] Retrieved ${contextArr.length} context snippet(s).`)
    const context = contextArr.join('\n---\n')
    this.logger.debug(`Retrieved external context: ${JSON.stringify(contextArr)}`)

    // Compose the memory/history block for the prompt
    const historyContext = history.map((m) => `${m.role === 'user' ? 'user' : 'system'}: ${m.content}`).join('\n')

    // Build prompt using language template
    const prompt = this.agentContent.buildPrompt({
      lang: userLang,
      context: `${historyContext}\n${context}`,
      question: userInput,
      userName,
    })
    this.logger.verbose(`Final prompt built for LLM:\n${prompt}`)

    // Call LLM to get response

    const answer = await this.llmService.generate(prompt, { session })
    this.logger.debug(`LLM generated answer: "${answer}"`)

    // Save both user question and agent reply to memory
    await this.memoryService.addMessage(connectionId, 'user', userInput)
    await this.memoryService.addMessage(connectionId, 'system', answer)
    this.logger.log(`History updated for session: ${connectionId}`)

    return answer
  }
}
